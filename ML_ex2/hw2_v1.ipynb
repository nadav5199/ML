{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bd0516e7cb654f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Decision Trees\n",
    "\n",
    "### Make sure that you read and fully understand all the guidelines listed below before you proceed with the exercise.\n",
    "\n",
    "* HW assignments are a significant part of the learning experience in this course and contribute 50% to your final grade. So, make sure to devote the appropriate time to them.\n",
    "* **Sharing solutions with someone who is not your submitting partner is strictly prohibited**. This includes reading someone else's code or sharing your code / posting it somewhere.\n",
    "* Appeals regarding submissions that do not follow the guidelines will not be accepted. \n",
    "\n",
    "\n",
    "### Guidelines for Programming Exercises:\n",
    "\n",
    "* Complete the required functions in `hw2.py`. Any modifications to this notebook will not be tested by our automated tests.\n",
    "* Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise can take several minutes when implemented efficiently, but will take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "* You are responsible for the correctness of your code. You can add tests to this jupyter notebook to validate your solution. The contents of this jupyter notebook will not be graded or checked.\n",
    "* You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/), numpy and pandas only. **Do not import anything else.**\n",
    "* Use `numpy` version 1.15.4 or higher.\n",
    "* Your code must run without errors. Code that cannot run will not be graded.\n",
    "* Your code will be tested using automated scripts. So, failure to follow the instructions may lead to test failure, which might significantly affect your grade. \n",
    "\n",
    "\n",
    "### Guidlines for Theoretical Exercises\n",
    "* Your solution should be written or typed and submitted in a separate file `hw2.pdf`.\n",
    "* If you scan a handwritten solution, make sure that your handwriting is legible and the scan quality is good.\n",
    "* You are expected to solve the questions analytically and provide a step-by-step solution. \n",
    "* It is okay and often recommended to use python to carry out the computations. \n",
    "* You may use the lecture slides and previous homework assignments as references, unless explicitly asked to prove a result from class. \n",
    "\n",
    "### Submission Guidelines:\n",
    "* Submit your solutiuon in a zip file that contains: \n",
    "  - The `hw2.py` script with your solution to the progamming exercise\n",
    "  - This notebook with your added tests (this is not checked or graded)\n",
    "  - The `hw2.pdf` file with your solution to the theoretical exercises.\n",
    "  \n",
    "* The name of the zip file should contain your ID(s). For example, `hw2_123456789_987654321.zip` if you submitted in pairs and `hw2_123456789.zip` if you submitted the exercise alone.\n",
    "* Please use **only a zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "### *** YOUR ID HERE ***\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theoretical Exercises (16 points)\n",
    "\n",
    "\n",
    "## 1. Gini Impurity\n",
    "In class, we defined the Gini impurity as \n",
    "$$\n",
    "\\varphi_{Gini}(p) = 1 - \\sum_{j=1}^k p_j^2,\\qquad p \\in [0,1]^k~,\n",
    "$$\n",
    "where $p=(p_1,\\ldots,p_k)$ represents class proportions in a set of instances. This means that $\\sum_{j=1}^kp_j = 1$. \n",
    "\n",
    "1. Prove that \n",
    "$$\n",
    "\\varphi_{Gini}(p) \\leq 1-1/k.\n",
    "$$\n",
    "Hint: \n",
    "- Express the function $f : \\mathbb R^{k-1} \\to \\mathbb R$:\n",
    "$$\n",
    "f(p_1,\\ldots,p_{k-1}) = \\varphi_{Gini}(p_1,\\ldots,1 - \\sum_{j=1}^k). \n",
    "$$\n",
    "- Argue that $f$ is bounded from above, hence it has a maximal value in $\\mathbb R^{k-1}$. \n",
    "- Solve the equation $\\nabla f = 0$ and argue that the solution is unique. \n",
    "\n",
    "(you do not have to follow the hint; all correct and clearly written solutions are acceptable)\n",
    "\n",
    "\n",
    "Let $Y_1$ and $Y_2$ be two independent random variables, each represnting the class label of a randomly sampled instance from the set. Namely:\n",
    "$$\n",
    " \\Pr[Y_i = j]=p_j, \\qquad i\\in\\{1,2\\}, \\qquad j\\in\\{1\\ldots k\\}~.\n",
    "$$ \n",
    "2. Prove that Gini impurity is the probability that two randomly sampled instances (with replacement) from the set of instances have different class labels. Namley, that\n",
    "$$\n",
    "\\varphi_{Gini}(p) = \\Pr[Y_1 \\neq  Y_2].\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Information Gain\n",
    "In class we claimed that **information gain is always non-negative**. Here, we will prove this for the specific case of binary classification, where we have only two class labels.\n",
    "\n",
    "Recall that information gain is defined as follows:\n",
    "$$\n",
    "IG(S,A) ~~=~~ H(S) - \\sum_{v\\in Values(A)} \\frac{|S_v|}{|S|}H(S_v)~,\n",
    "$$\n",
    "where $S$ is a set of data instances, $A$ is an attribute (ferature) with a finite set of possible values $Values(A)$, and $H$ is the entropy function applied to the probability vector associated with the class frequencies. Assuming that there are only two class lables, the entropy can be expressed as follows:\n",
    "$$\n",
    "H(S)  ~~=~~ h(p_1) ~~=~~ -p_1\\log(p_1)-(1-p_1)\\log(1-p_1)~,\n",
    "$$\n",
    "where $p_1$ is the frequency of the first label (and $1-p_1$ is the frequency of the second label). Here, we adhere to the convention that $0\\cdot \\log(0) = 0$ (as $\\log(0)$ is undefined). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by examining the function $h()$, which is also called the *binary entropy function* (see plot below). One feature of this function is that it is *concave*. Concave functions satisfy the following property: for every $x_1,x_2\\in[0,1]$ and for every $\\lambda_1,\\lambda_2\\in[0,1]$ such that $\\lambda_1+\\lambda_2=1$, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "h(\\lambda_1 x_1 +  \\lambda_2 x_2) \\geq \\lambda_1 h(x_1) + \\lambda_2 h(x_2). \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The plot below illustrates this inequality, whose correctness we will assume here without a formal proof (this can be proved by analysis of the first and second derivatives of $h()$).\n",
    "![Entropy-concave](entropy-concave.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the inequality in (1) to prove (by induction) a more general claim: for any $t\\geq 2$ points $x_1\\ldots x_t\\in [0,1]$, and $t$ weights $\\lambda_1\\ldots \\lambda_t\\in [0,1]$ such that $\\sum_{j=1}^t\\lambda_j = 1$, we have\n",
    "$$\n",
    "h\\left(\\sum_{j=1}^t\\lambda_jx_j\\right) \\geq \\sum_{j=1}^t\\lambda_jh(x_j)~.\n",
    "$$\n",
    "This inequality, which applies to all concave functions, is also called *Jensen's inequality*.\n",
    "\n",
    "2. Use the inequality you proved above to prove that information gain is always non-negative (when there are only two classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Coding Assignment (84 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed9fe7b1026e33cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6ac605270c2b091",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Warmup - OOP in python\n",
    "\n",
    "Our desicion tree will be implemented using a dedicated python class. Python classes are very similar to classes in other object oriented programming languages you might be familiar with. You can use the following [site](https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/) to learn about classes in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Node(5)\n",
    "p = Node(6)\n",
    "q = Node(7)\n",
    "n.add_child(p)\n",
    "n.add_child(q)\n",
    "n.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1ceb251c649b62",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2. Data Exploration and Preprocessing\n",
    "\n",
    "### Data Exploration\n",
    "For the following exercise, we will use a dataset containing information on mushrooms. This dataset includes descriptions of hypothetical samples from 23 species of gilled mushrooms in the Agaricus and Lepiota genera. Each sample is identified as being \"definitely edible\", \"definitely poisonous\", or of \"unknown edibility\". Here, we will be conservative and will group the species of \"unknown edibility\" with the poisonous ones. Thus, we will have two classes: **edible** and **poisonous**. \n",
    "    \n",
    "Each sample is also provided with information on the following 21 attributes (features):\n",
    "1. cap-shape: bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s\n",
    "1. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "1. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "1. bruises: bruises=t,no=f\n",
    "1. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\n",
    "1. gill-attachment: attached=a,descending=d,free=f,notched=n\n",
    "1. gill-spacing: close=c,crowded=w,distant=d\n",
    "1. gill-size: broad=b,narrow=n\n",
    "1. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "1. stalk-shape: enlarging=e,tapering=t\n",
    "1. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "1. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "1. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "1. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "1. veil-type: partial=p,universal=u\n",
    "1. veil-color: brown=n,orange=o,white=w,yellow=y\n",
    "1. ring-number: none=n,one=o,two=t\n",
    "1. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "1. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "1. population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "1. habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n",
    "\n",
    "The table in `agaricus-lepiota.csv` contains feature and class information on 8124 mushroom samples. We start by reading the data using the `.read_csv` method from ``pandas``, and then visually the top and bottom of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79cb4542926ad3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('agaricus-lepiota.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of the Decision Tree algorithm is that almost no preprocessing is required. However, finding missing values is always important. Use the following command to confirm that there are no missing values in the data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "As in all machine learning tasks, we split the dataset to `training` (75%) and `validation` (25%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Making sure the last column will hold the labels\n",
    "X, y = data.drop('class', axis=1), data['class']\n",
    "X = np.column_stack([X,y])\n",
    "# split dataset using random_state to get the same split each time\n",
    "X_train, X_validation = train_test_split(X, random_state=99)\n",
    "\n",
    "print(\"Training dataset shape: \", X_train.shape)\n",
    "print(\"Validation dataset shape: \", X_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd7b0191f3f1e897",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2. Impurity Measures (10 points)\n",
    "\n",
    "Impurity measures play a central role in the tree construction algorithm, as they measure how far a set of samples is from being fully classified. We discussed two impurity measures in class: *Gini* and *Entropy*. Implement the functions `calc_gini` and `calc_entropy` in `hw2.py`. You are encouraged to test your implementation according to the expected behavior of those measures as seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2 import calc_gini, calc_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "calc_gini(X), calc_entropy(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing A Class for Nodes in the Decision Tree (35 points)\n",
    "\n",
    "Complete the implementation of the Python class `DecisionNode` in `hw2.py`. Follow these guidelines:\n",
    "1. Implement the member functions based on their description below and their signatures as specified in `hw2.py`.\n",
    "1. Implementation details are up to you, but maintain the function signatures and interface. \n",
    "1. **Do not change exisiting variables and function implentations**. \n",
    "1. You are allowed to add methods and variables and implement additional auxiliary functions.\n",
    "\n",
    "Furthermore, you may assume that all features are discrete and enumerable. Thus, the set of possible values for each feature can be obtained by computing the unique values of that feature in the data.\n",
    "\n",
    "A `DecisionNode` object has the following attributes (already defined in `hw2.py`):\n",
    "* `data` holds the data instances associated with the node (ndarray).\n",
    "* `terminal` True iff the node is a leaf (boolean).\n",
    "* `feature` holds the column index of feature/attribute to split upon (int).\n",
    "* `pred` holds the class prediction associated with the node (string).\n",
    "* `depth` holds the depth of the node (int).\n",
    "* `children` holds the children of the node (list of DecisionNode objects).\n",
    "* `children_values` holds the value associated with each child for the feature used for splitting the node (list).\n",
    "* `max_depth` holds the maximum allowed depth of the entire tree (int).\n",
    "* `chi` holds the P-value cutoff used for $\\chi^2$ pruning (double) (see below).\n",
    "* `impurity_func` holds the impurity function to use for measuring goodness of a split (func).\n",
    "* `gain_ratio` True iff GainRatio is used to score features (boolean).\n",
    "* `feature_importance` holds the feature importance of the chosen feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Methods (10 points)\n",
    "\n",
    "Implement the methods `calc_node_pred` and `add_child` of the `DecisionNode` class in `hw2.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of Split (10 Points)\n",
    "\n",
    "Implement the method `goodness_of_split` of the `DecisionNode` class in `hw2.py`.\n",
    "\n",
    "Goodnees of split allows us to prioritize different attributes when splitting the decision node. If the `gain_ratio` flag is set to `False` (its default value), then goodness of the split of a node associated with data $S$ with attribute $A$ is measured by impurity reduction defiend as follows:\n",
    "$$\n",
    "GOS(S,A) ~~=~~ \\Delta\\varphi(S, A) ~~=~~ \\varphi(S) - \\sum_{v\\in Values(A)} \\frac{|S_v|}{|S|}\\varphi(S_v)~,\n",
    "$$\n",
    "where $\\varphi$ is an impurity function (Gini or Entropy) based on the `impurity_func` attribute of the `DecisionNode` object.\n",
    "\n",
    "If the `gain_ratio` flag is set to `True`, then goodness of a split is measured using gain ratio as follows:\n",
    "$$\n",
    "GOS(S,A) ~~=~~ GainRatio(S,A) ~~=~~ \\frac{IG(S,A)}{SplitInfo(S,A)}~~,\n",
    "$$\n",
    "where $IG(S,A)$ (the information gain) is the impurity reduction computed using entropy, and $SplitInfo(S,A)$ is defined as follows:\n",
    "$$\n",
    "SplitInfo(S,A)=- \\sum_{v\\in Values(A)} \\frac{|S_v|}{|S|}\\log\\left(\\frac{|S_v|}{|S|}\\right) ~.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance (5 points) \n",
    "\n",
    "Implement the method `calc_feature_importance` of the `DecisionNode` class in `hw2.py`.\n",
    "\n",
    "Feature importance of a given node is calculated as its goodness of split multiplied by the node's relative weight. The node's relative weight is defined as the ratio between the number of  training samples associated with the node ($S$) and the size of the training set $|S_{train}|$. Thus, the feature importance of a node associated with data samples $S$ and attribute $A$ is defined as:\n",
    "$$\n",
    "FI(S,A) = \\frac{|S|}{|S_{train}|}GOS(S, A)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Node (10 points)\n",
    "\n",
    "Implement the method `split` of the `DecisionNode` class in `hw2.py`.\n",
    "\n",
    "This method finds the feature with highest \"goodness of split\" and uses it to split the node. Do not implement node pruning at this stage (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2 import DecisionNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "# python support passing a function as arguments to another function.\n",
    "gini_node = DecisionNode(X, calc_gini)\n",
    "entropy_node = DecisionNode(X, calc_entropy)\n",
    "goodness_gini, split_values_gini = gini_node.goodness_of_split(0)\n",
    "goodness_entropy, split_values_entropy = entropy_node.goodness_of_split(0)\n",
    "\n",
    "goodness_gini, goodness_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement A Class for Decision Trees (20 points)\n",
    "\n",
    "Complete the implementation of the Python class `DecisionTree` in `hw2.py`. Follow these guidelines:\n",
    "1. Implement the member functions based on their description below and their signatures as specified in `hw2.py`.\n",
    "1. Implementation details are up to you, but maintain the function signatures and interface. \n",
    "1. **Do not change exisiting variables and function implentations**. \n",
    "1. You are allowed to add methods and variables and implement additional auxiliary functions.\n",
    "\n",
    "A `DecisionTree` object has the following attributes (already defined in `hw2.py`):\n",
    "* `data` holds the training data used for tree construction (ndarray).\n",
    "* `root` holds the root node of the decision tree (`DecisionNode`)\n",
    "* `max_depth` holds the maximum allowed depth of the entire tree (int).\n",
    "* `chi` holds the P-value cutoff used for $\\chi^2$ pruning (double) (see below).\n",
    "* `impurity_func` holds the impurity function to use for measuring goodness of a split (func).\n",
    "* `gain_ratio` True iff GainRatio is used to score goodness of split (boolean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Construction (10 points)\n",
    "\n",
    "Implement the method `build_tree` of the `DecisionTree` class in `hw2.py`.\n",
    "\n",
    "The tree should be built recursively from the root. Compute the feature importance of all nodes in the tree (either during construction or after it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2 import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below constructs three decision trees using the training data and three different measures for goodness of split: Gini, Entropy, and Gain Ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "tree_gini = DecisionTree(data=X_train, impurity_func=calc_gini) # gini and goodness of split\n",
    "tree_gini.build_tree()\n",
    "\n",
    "tree_entropy = DecisionTree(data=X_train, impurity_func=calc_entropy) # entropy and goodness of split\n",
    "tree_entropy.build_tree()\n",
    "\n",
    "tree_entropy_gain_ratio = DecisionTree(data=X_train, impurity_func=calc_entropy, gain_ratio=True) # entropy and gain ratio\n",
    "tree_entropy_gain_ratio.build_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Prediction and Tree Evaluation (10 points) \n",
    "\n",
    "Implement the methods `predict` and `calc_accuracy` of the `DecisionTree` class in `hw2.py`.\n",
    "\n",
    "The code below calculates and prints the prediction accuracy of the three trees we constructed on the training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "print('gini', tree_gini.calc_accuracy(X_train), tree_gini.calc_accuracy(X_validation))\n",
    "print('entropy', tree_entropy.calc_accuracy(X_train), tree_entropy.calc_accuracy(X_validation))\n",
    "print('entropy gain ratio', tree_entropy_gain_ratio.calc_accuracy(X_train), \n",
    "      tree_entropy_gain_ratio.calc_accuracy(X_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Depth pruning (5 points)\n",
    "\n",
    "Now, we wish to prune the decision tree to different depths and examine the influence on prediction accuracy on the training data and validation data. \n",
    "\n",
    "Implement the function `depth_pruning` in `hw2.py`.\n",
    "\n",
    "This function should construct trees with `max_depth` in the range `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]` and calculate the training and validation accuracy of these trees. When constructing each individual tree, use the best goodness of split method you found above, and do not let the tree depth exceed the specified value of `max_depth`. Note that you might need to modify methods in the `DecisionNode` and/or `DecisionTree` classes.\n",
    "\n",
    "In order to debug your code, plot the training and validation accuracy as a function of the `max_depth` and verify that your results make sense. The model with highest validation accuracy is marked in red.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "from hw2 import depth_pruning\n",
    "depth_training_acc, depth_validation_acc = depth_pruning(X_train, X_validation)\n",
    "\n",
    "plt.plot(range(1, 11), depth_training_acc, label='Training')\n",
    "plt.plot(range(1, 11), depth_validation_acc, label='Validation')\n",
    "plt.scatter(np.argmax(depth_validation_acc)+1, max(depth_validation_acc), c='r')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. $\\chi^2$ pruning (10 points)\n",
    "\n",
    "Now, we wish to prune the decision tree using the $\\chi^2$ (chi-squared) test.\n",
    "\n",
    "Implement the function `chi_pruning` in `hw2.py`.\n",
    "\n",
    "This function should construct trees while pruning nodes using P-value cut-off values in the range `[1, 0.5, 0.25, 0.1, 0.05, 0.0001]` (where 1 indicates no pruning). It then calculates and returns the training and validation accuracy of these trees. Note that you might need to modify methods in the `DecisionNode` and/or `DecisionTree` classes.\n",
    "\n",
    "In order to debug your code, plot the training and validation accuracy as a function of the tuple (P-value, tree-depth) and verify that your results make sense. The model with highest validation accuracy is marked in red.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2 import chi_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "chi_training_acc, chi_validation_acc, depth = chi_pruning(X_train, X_validation)\n",
    "\n",
    "chi_depth_tuple = [str((x, y)) for x, y in zip([1, 0.5, 0.25, 0.1, 0.05, 0.0001], depth)][::-1]\n",
    "plt.plot(chi_depth_tuple, chi_training_acc[::-1], label='Training')\n",
    "plt.plot(chi_depth_tuple, chi_validation_acc[::-1], label='Validation')\n",
    "plt.scatter(chi_depth_tuple[np.argmax(chi_validation_acc[::-1])], max(chi_validation_acc), c='r')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, modify the code below to construct the two best trees based on the two pruning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree with highest validation accuracy based on depth pruning\n",
    "tree_max_depth = None\n",
    "# Tree with highest validation accuracy based on chi-squared pruning\n",
    "tree_chi = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Selecting Smallest Tree (4 points) \n",
    "\n",
    "Of the two trees above, we wish to choose the one with fewer nodes. Implement the function `count_nodes` in `hw2.py`. This function counts the number of nodes in a subtree rooted by a given `DecisionNode` object (including that node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2 import count_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the two lines of code below to print the sizes of the two trees you constructed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "# print(count_nodes(tree_max_depth.root))\n",
    "# print(count_nodes(tree_chi.root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Print the tree\n",
    "\n",
    "We provided you with a function that should print your tree for your own debugging purposes. \n",
    "\n",
    "This code prints:\n",
    "```\n",
    "[ROOT, feature=X0],\n",
    "  [X0=a, feature=X2]\n",
    "    [X2=c, leaf]: [{1.0: 10}]\n",
    "    [X2=d, leaf]: [{0.0: 10}]\n",
    "  [X0=y, feature=X5], \n",
    "       [X5=a, leaf]: [{1.0: 5}]\n",
    "       [X5=s, leaf]: [{0.0: 10}]\n",
    "  [X0=e, leaf]: [{0.0: 25, 1.0: 50}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0, parent_feature='ROOT', feature_val='ROOT'):\n",
    "    '''\n",
    "    prints the tree according to the example above\n",
    "\n",
    "    Input:\n",
    "    - node: a node in the decision tree\n",
    "\n",
    "    This function has no return value\n",
    "    '''\n",
    "    if node.terminal == False:\n",
    "        if node.depth == 0:\n",
    "            print('[ROOT, feature=X{}, importance={:.3f}]'.format(node.feature, node.feature_importance))\n",
    "        else:\n",
    "            print('{}[X{}={}, feature=X{}, importance={:.3f}], Depth: {}'.format(depth*'  ', parent_feature, \n",
    "                                                                             feature_val, node.feature, \n",
    "                                                                             node.feature_importance, node.depth))\n",
    "        for i, child in enumerate(node.children):\n",
    "            print_tree(child, depth+1, node.feature, node.children_values[i])\n",
    "    else:\n",
    "        classes_count = {}\n",
    "        labels, counts = np.unique(node.data[:, -1], return_counts=True)\n",
    "        for l, c in zip(labels, counts):\n",
    "            classes_count[l] = c\n",
    "        print('{}[X{}={}, leaf]: [{}], Depth: {}'.format(depth*'  ', parent_feature, feature_val,\n",
    "                                                         classes_count, node.depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree_entropy_gain_ratio.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
